# Database Configuration
# Get this from your Neon project dashboard
DATABASE_URL="postgresql://username:password@host.neon.tech/dbname?sslmode=require"

# Ollama Configuration
# Local Ollama endpoint (default is localhost:11434)
OLLAMA_HOST=http://localhost:11434

# Models to use
OLLAMA_EMBED_MODEL=nomic-embed-text
OLLAMA_LLM_MODEL=phi3.5

# API Keys
# Groq API for LLM inference
GROQ_API_KEY=your_groq_api_key_here

# OpenRouter API for LLM inference
OPENROUTER_API_KEY=your_openrouter_api_key_here

# LLM Cascade Strategy
# "extended" = Gemini -> Groq -> Ollama (faster, more resilient, default)
# "simple" = Gemini -> Ollama (preserves Groq rate limits)
LLM_CASCADE_STRATEGY=extended

# Pipeline Configuration
# Batch size for processing sections
PIPELINE_BATCH_SIZE=1000

# Loader Configuration
# Batch size for database inserts
LOADER_BATCH_SIZE=500

# Next.js Configuration (for apps/web/.env.local)
# Copy DATABASE_URL from above
NEXT_PUBLIC_APP_URL=http://localhost:3000
